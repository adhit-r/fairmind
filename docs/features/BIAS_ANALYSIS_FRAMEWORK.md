# Comprehensive Bias Analysis Framework for AI Governance

## üéØ **Overview**

This document outlines a comprehensive framework for detecting, analyzing, and mitigating bias in machine learning models, based on industry best practices and research from Google's Machine Learning Crash Course.

## üîç **Types of Bias in Machine Learning**

### **1. Reporting Bias**
**Definition**: When the frequency of events, properties, and/or outcomes captured in a dataset does not accurately reflect their real-world frequency.

**Example**: A sentiment-analysis model trained on book reviews where most reviews reflect extreme opinions (loved or hated), because people were less likely to submit reviews for books they felt neutral about.

**Detection Methods**:
- Analyze distribution of target variable values
- Compare dataset distributions to real-world statistics
- Check for over-representation of extreme cases
- Examine data collection methodology

### **2. Historical Bias**
**Definition**: When historical data reflects inequities that existed in the world at that time.

**Example**: A city housing dataset from the 1960s containing home-price data that reflects discriminatory lending practices from that era.

**Detection Methods**:
- Analyze temporal patterns in data
- Compare historical vs. current distributions
- Check for outdated societal norms in data
- Examine data collection time periods

### **3. Automation Bias**
**Definition**: A tendency to favor results generated by automated systems over those generated by non-automated systems, irrespective of error rates.

**Example**: ML practitioners eager to deploy a "groundbreaking" model for defect detection, despite the model's precision and recall being 15% lower than human inspectors.

**Detection Methods**:
- Compare automated vs. manual system performance
- Analyze decision-making patterns
- Check for over-reliance on automated outputs
- Examine human-AI interaction patterns

### **4. Selection Bias**
**Definition**: When a dataset's examples are chosen in a way that is not reflective of their real-world distribution.

#### **4.1 Coverage Bias**
**Definition**: Data is not selected in a representative fashion.

**Example**: A model trained on phone surveys of consumers who bought a product, excluding those who bought competing products.

#### **4.2 Non-Response Bias**
**Definition**: Data becomes unrepresentative due to participation gaps in data collection.

**Example**: Consumers who bought competing products were 80% more likely to refuse surveys, leading to underrepresentation.

#### **4.3 Sampling Bias**
**Definition**: Proper randomization is not used during data collection.

**Example**: Surveyors chose the first 200 consumers who responded to an email, who might be more enthusiastic than average purchasers.

### **5. Group Attribution Bias**
**Definition**: A tendency to generalize what is true of individuals to the entire group they belong to.

#### **5.1 In-Group Bias**
**Definition**: Preference for members of your own group or characteristics you share.

**Example**: ML practitioners training a r√©sum√©-screening model believing applicants from their alma mater are more qualified.

#### **5.2 Out-Group Homogeneity Bias**
**Definition**: Tendency to stereotype individual members of groups you don't belong to.

**Example**: Believing all applicants who didn't attend a computer-science academy lack sufficient expertise.

### **6. Implicit Bias**
**Definition**: Assumptions made based on one's own model of thinking and personal experiences.

**Example**: Using head shake as "no" in gesture recognition, when in some regions it means "yes."

### **7. Confirmation Bias**
**Definition**: Unconsciously processing data in ways that affirm pre-existing beliefs.

**Example**: An ML practitioner with negative childhood experience with toy poodles unconsciously discarding features showing docility in smaller dogs.

### **8. Experimenter's Bias**
**Definition**: Training a model until it produces results aligning with original hypothesis.

**Example**: Retraining a model multiple times until it shows smaller poodles as more violent, confirming pre-existing beliefs.

## üîç **Bias Detection Methods**

### **1. Data Quality Analysis**

#### **Missing Feature Values**
**Red Flags**:
- Large number of missing values for specific features
- Missing values correlated with certain groups
- Systematic patterns in missing data

**Detection**:
```python
def analyze_missing_values(df, sensitive_attributes):
    """Analyze missing values for potential bias"""
    missing_analysis = {}
    
    for attr in sensitive_attributes:
        missing_by_group = df.groupby(attr).apply(
            lambda x: x.isnull().sum() / len(x)
        )
        missing_analysis[attr] = missing_by_group
    
    return missing_analysis
```

#### **Unexpected Feature Values**
**Red Flags**:
- Outliers that don't make sense
- Impossible or unlikely values
- Values outside expected ranges

**Detection**:
```python
def detect_unexpected_values(df, feature_ranges):
    """Detect unexpected feature values"""
    unexpected = {}
    
    for feature, (min_val, max_val) in feature_ranges.items():
        outliers = df[
            (df[feature] < min_val) | (df[feature] > max_val)
        ]
        unexpected[feature] = outliers
    
    return unexpected
```

#### **Data Skew Analysis**
**Red Flags**:
- Under-representation of certain groups
- Over-representation of majority groups
- Imbalanced distributions

**Detection**:
```python
def analyze_data_skew(df, sensitive_attributes):
    """Analyze data skew across sensitive attributes"""
    skew_analysis = {}
    
    for attr in sensitive_attributes:
        group_counts = df[attr].value_counts()
        total = len(df)
        
        skew_analysis[attr] = {
            'distribution': group_counts / total,
            'imbalance_ratio': group_counts.max() / group_counts.min(),
            'minority_group_size': group_counts.min()
        }
    
    return skew_analysis
```

### **2. Model Performance Analysis**

#### **Subgroup Performance Analysis**
```python
def analyze_subgroup_performance(model, test_data, sensitive_attributes):
    """Analyze model performance across subgroups"""
    performance_by_group = {}
    
    for attr in sensitive_attributes:
        groups = test_data[attr].unique()
        
        for group in groups:
            group_data = test_data[test_data[attr] == group]
            predictions = model.predict(group_data)
            
            performance_by_group[f"{attr}_{group}"] = {
                'accuracy': accuracy_score(group_data['target'], predictions),
                'precision': precision_score(group_data['target'], predictions),
                'recall': recall_score(group_data['target'], predictions),
                'f1_score': f1_score(group_data['target'], predictions),
                'sample_size': len(group_data)
            }
    
    return performance_by_group
```

#### **Fairness Metrics Calculation**

##### **Demographic Parity**
```python
def calculate_demographic_parity(predictions, sensitive_attr):
    """Calculate demographic parity across groups"""
    groups = sensitive_attr.unique()
    acceptance_rates = {}
    
    for group in groups:
        group_mask = sensitive_attr == group
        group_predictions = predictions[group_mask]
        acceptance_rates[group] = np.mean(group_predictions)
    
    # Calculate disparity
    max_rate = max(acceptance_rates.values())
    min_rate = min(acceptance_rates.values())
    disparity = max_rate - min_rate
    
    return {
        'acceptance_rates': acceptance_rates,
        'disparity': disparity,
        'ratio': max_rate / min_rate if min_rate > 0 else float('inf')
    }
```

##### **Equality of Opportunity**
```python
def calculate_equality_of_opportunity(predictions, targets, sensitive_attr):
    """Calculate equality of opportunity for positive class"""
    groups = sensitive_attr.unique()
    opportunity_rates = {}
    
    for group in groups:
        group_mask = sensitive_attr == group
        group_predictions = predictions[group_mask]
        group_targets = targets[group_mask]
        
        # Only consider positive examples
        positive_mask = group_targets == 1
        if positive_mask.sum() > 0:
            positive_predictions = group_predictions[positive_mask]
            opportunity_rates[group] = np.mean(positive_predictions)
        else:
            opportunity_rates[group] = 0
    
    # Calculate disparity
    max_rate = max(opportunity_rates.values())
    min_rate = min(opportunity_rates.values())
    disparity = max_rate - min_rate
    
    return {
        'opportunity_rates': opportunity_rates,
        'disparity': disparity,
        'ratio': max_rate / min_rate if min_rate > 0 else float('inf')
    }
```

##### **Counterfactual Fairness**
```python
def analyze_counterfactual_fairness(model, test_data, sensitive_attr):
    """Analyze counterfactual fairness for similar examples"""
    counterfactual_pairs = []
    
    # Find pairs of examples that are identical except for sensitive attribute
    for i in range(len(test_data)):
        for j in range(i + 1, len(test_data)):
            example1 = test_data.iloc[i]
            example2 = test_data.iloc[j]
            
            # Check if examples are identical except for sensitive attribute
            if are_examples_identical_except_sensitive(example1, example2, sensitive_attr):
                pred1 = model.predict([example1.drop(sensitive_attr)])
                pred2 = model.predict([example2.drop(sensitive_attr)])
                
                counterfactual_pairs.append({
                    'example1': example1,
                    'example2': example2,
                    'prediction1': pred1[0],
                    'prediction2': pred2[0],
                    'fair': pred1[0] == pred2[0],
                    'difference': abs(pred1[0] - pred2[0])
                })
    
    return counterfactual_pairs
```

## üõ†Ô∏è **Bias Mitigation Strategies**

### **1. Data Augmentation**

#### **Oversampling Minority Groups**
```python
def oversample_minority_groups(df, sensitive_attr, target_col):
    """Oversample minority groups to balance dataset"""
    from imblearn.over_sampling import SMOTE
    
    # Identify minority groups
    group_counts = df[sensitive_attr].value_counts()
    minority_groups = group_counts[group_counts < group_counts.mean()]
    
    balanced_df = df.copy()
    
    for group in minority_groups.index:
        group_data = df[df[sensitive_attr] == group]
        
        if len(group_data) > 0:
            # Use SMOTE to generate synthetic samples
            smote = SMOTE(random_state=42)
            X = group_data.drop([sensitive_attr, target_col], axis=1)
            y = group_data[target_col]
            
            X_resampled, y_resampled = smote.fit_resample(X, y)
            
            # Create synthetic samples
            synthetic_data = pd.DataFrame(X_resampled, columns=X.columns)
            synthetic_data[sensitive_attr] = group
            synthetic_data[target_col] = y_resampled
            
            # Add to balanced dataset
            balanced_df = pd.concat([balanced_df, synthetic_data])
    
    return balanced_df
```

#### **Feature Engineering for Fairness**
```python
def engineer_fair_features(df, sensitive_attr):
    """Engineer features to reduce bias"""
    df_fair = df.copy()
    
    # Create interaction features
    for col in df.columns:
        if col != sensitive_attr and df[col].dtype in ['int64', 'float64']:
            df_fair[f'{col}_interaction'] = df[col] * df[sensitive_attr]
    
    # Create fairness-aware features
    df_fair['group_awareness'] = df[sensitive_attr].map(
        df.groupby(sensitive_attr).size() / len(df)
    )
    
    return df_fair
```

### **2. Model-Level Mitigation**

#### **MinDiff Implementation**
```python
def apply_mindiff_mitigation(base_model, train_data, sensitive_attr, target_col):
    """Apply MinDiff bias mitigation"""
    from tensorflow_model_remediation import min_diff
    
    # Create MinDiff model
    mindiff_model = min_diff.keras.MinDiffModel(
        original_model=base_model,
        loss=min_diff.losses.MMDLoss(),
        loss_weight=1.0
    )
    
    # Prepare MinDiff data
    sensitive_group = train_data[
        (train_data[sensitive_attr] == 1) & (train_data[target_col] == 1)
    ]
    non_sensitive_group = train_data[
        (train_data[sensitive_attr] == 0) & (train_data[target_col] == 1)
    ]
    
    # Pack data for MinDiff
    mindiff_data = min_diff.keras.utils.pack_min_diff_data(
        original_dataset=train_data,
        sensitive_group_dataset=sensitive_group,
        nonsensitive_group_dataset=non_sensitive_group
    )
    
    return mindiff_model, mindiff_data
```

#### **Counterfactual Logit Pairing**
```python
def apply_clp_mitigation(base_model, train_data, sensitive_attr):
    """Apply Counterfactual Logit Pairing mitigation"""
    from tensorflow_model_remediation import counterfactual
    
    # Create CLP model
    clp_model = counterfactual.keras.CounterfactualModel(
        original_model=base_model,
        loss=counterfactual.losses.CounterfactualLoss(),
        loss_weight=1.0
    )
    
    # Find counterfactual pairs
    counterfactual_pairs = find_counterfactual_pairs(
        train_data, sensitive_attr
    )
    
    return clp_model, counterfactual_pairs
```

### **3. Post-Processing Mitigation**

#### **Threshold Adjustment**
```python
def adjust_thresholds_for_fairness(model, test_data, sensitive_attr, target_col):
    """Adjust prediction thresholds to achieve fairness"""
    thresholds = {}
    
    for group in test_data[sensitive_attr].unique():
        group_data = test_data[test_data[sensitive_attr] == group]
        
        # Get predictions
        predictions = model.predict_proba(group_data.drop([sensitive_attr, target_col], axis=1))
        
        # Find optimal threshold for this group
        from sklearn.metrics import roc_curve
        fpr, tpr, threshold = roc_curve(
            group_data[target_col], 
            predictions[:, 1]
        )
        
        # Choose threshold that maximizes fairness
        optimal_threshold = threshold[np.argmax(tpr - fpr)]
        thresholds[group] = optimal_threshold
    
    return thresholds
```

#### **Rejection Option Classification**
```python
def apply_rejection_option(model, test_data, confidence_threshold=0.1):
    """Apply rejection option for uncertain predictions"""
    predictions = model.predict_proba(test_data)
    confidence = np.max(predictions, axis=1)
    
    # Reject predictions with low confidence
    rejected_mask = confidence < confidence_threshold
    accepted_predictions = predictions[~rejected_mask]
    
    return {
        'accepted_predictions': accepted_predictions,
        'rejected_indices': np.where(rejected_mask)[0],
        'rejection_rate': rejected_mask.mean()
    }
```

## üìä **Bias Analysis Dashboard Components**

### **1. Data Quality Dashboard**
```typescript
interface DataQualityMetrics {
  missing_values: {
    overall_rate: number
    by_feature: Record<string, number>
    by_group: Record<string, Record<string, number>>
  }
  unexpected_values: {
    outliers: Record<string, number>
    impossible_values: Record<string, number>
  }
  data_skew: {
    group_distributions: Record<string, Record<string, number>>
    imbalance_ratios: Record<string, number>
    minority_group_sizes: Record<string, number>
  }
}
```

### **2. Model Performance Dashboard**
```typescript
interface ModelPerformanceMetrics {
  overall_metrics: {
    accuracy: number
    precision: number
    recall: number
    f1_score: number
  }
  subgroup_performance: Record<string, {
    accuracy: number
    precision: number
    recall: number
    f1_score: number
    sample_size: number
  }>
  fairness_metrics: {
    demographic_parity: {
      acceptance_rates: Record<string, number>
      disparity: number
      ratio: number
    }
    equality_of_opportunity: {
      opportunity_rates: Record<string, number>
      disparity: number
      ratio: number
    }
    counterfactual_fairness: {
      total_pairs: number
      fair_pairs: number
      unfair_pairs: number
      average_difference: number
    }
  }
}
```

### **3. Bias Mitigation Dashboard**
```typescript
interface BiasMitigationMetrics {
  data_augmentation: {
    original_distribution: Record<string, number>
    augmented_distribution: Record<string, number>
    improvement: Record<string, number>
  }
  model_mitigation: {
    before_metrics: ModelPerformanceMetrics
    after_metrics: ModelPerformanceMetrics
    improvement: Record<string, number>
  }
  post_processing: {
    threshold_adjustments: Record<string, number>
    rejection_rates: Record<string, number>
    fairness_improvement: Record<string, number>
  }
}
```

## üéØ **Implementation Roadmap**

### **Phase 1: Basic Bias Detection (Weeks 1-2)**
- ‚úÖ **Data Quality Analysis**: Missing values, unexpected values, data skew
- ‚úÖ **Basic Fairness Metrics**: Demographic parity, equality of opportunity
- ‚úÖ **Subgroup Performance Analysis**: Performance across sensitive attributes

### **Phase 2: Advanced Bias Analysis (Weeks 3-4)**
- üîÑ **Counterfactual Fairness**: Individual example analysis
- üîÑ **Bias Type Classification**: Automatic identification of bias types
- üîÑ **Temporal Bias Analysis**: Historical bias detection

### **Phase 3: Bias Mitigation (Weeks 5-6)**
- üìã **Data Augmentation**: SMOTE, oversampling, feature engineering
- üìã **Model-Level Mitigation**: MinDiff, CLP implementation
- üìã **Post-Processing**: Threshold adjustment, rejection option

### **Phase 4: Comprehensive Dashboard (Weeks 7-8)**
- üìã **Interactive Visualizations**: Bias analysis charts and graphs
- üìã **Automated Reports**: Bias detection and mitigation reports
- üìã **Recommendation Engine**: Automated bias mitigation suggestions

## üöÄ **Key Benefits**

### **For Data Scientists**
- **Comprehensive Bias Detection**: Identify all types of bias systematically
- **Automated Analysis**: Reduce manual bias detection effort
- **Mitigation Strategies**: Proven techniques for bias reduction
- **Performance Tracking**: Monitor bias reduction over time

### **For Organizations**
- **Regulatory Compliance**: Meet fairness requirements
- **Risk Mitigation**: Reduce bias-related risks
- **Reputation Protection**: Build trust with stakeholders
- **Competitive Advantage**: Demonstrate responsible AI practices

### **For Society**
- **Fair AI Systems**: Reduce discrimination in AI applications
- **Transparency**: Better understanding of AI decision-making
- **Accountability**: Clear metrics for AI fairness
- **Inclusion**: More equitable AI systems

## üéØ **Conclusion**

This comprehensive bias analysis framework provides a systematic approach to detecting, analyzing, and mitigating bias in machine learning models. By implementing these techniques, organizations can build more fair, transparent, and responsible AI systems that benefit all stakeholders.

The framework is designed to be:
- **Comprehensive**: Covers all major types of bias
- **Practical**: Provides implementable solutions
- **Scalable**: Works across different model types and domains
- **Measurable**: Includes clear metrics for success

For Fairmind, this framework will be integrated into the bias detection dashboard to provide users with industry-leading bias analysis capabilities.
